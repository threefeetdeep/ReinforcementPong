This is a log of the steps taken to create my Reinforcement Learning Pong game in Python.

I would like to document this on my website as concisely as possible.

I will intially aim to use pygame for the game mechanics, and sklearnfor the machine
learning aspects.

1) Created folder C:\Users\oli.bailey\Documents\Code\Python\PongRL
2) Downloaded seven segment font (.ttf) for use in the score board in game
3) Created python3 virtual env:
    python3 -m venv env
4) Start the environment
    env\Scripts\activate
5) update pip3
    python3 -m pip install --upgrade pip
6) Download required python packages
    pip3 install pygame matplotlib sklearn 

7) Create the game 'pong.py' with the option to accept human or agent control

8) REINFORCEMENT LEARNING RE-CAP:

The reward signal is to tell our agent *what* we want it to achieve, not *how* to 
do it. 

E.g. in chess, reward winning the game, not e.g. subrewards like capturing enemy pieces.
Otherwise the agent might just captured pieces even at the expense of losing the game!

Pong is an *episodic task* in that each game finishes with a certain reward, and the agent's
goal is to maximize the long term expected reward (i.e. for rewards in future games yet to happen)

Gt = ( R_(t+1) + R_(t+2) + ... + R_(T) )/n  

where Ri is the reward at the end of each future episode or game, and n is the number of games played,
and we are currently at episode t, finishing at episode T > t.

In Ping, natural rewards are scoring goals, losing goals, and winning/losing the game.


PROGRESS:
---------
First version with 2 states (ball-to-P1-paddle x and ball-to-P1-paddle y distances) seems
to train OK for the first 10-20 games, but then agent soon develops the habit of getting stuck in the top or mostly bottom corners!

Changing the activation functions from relu to sigmoid doesn't seem to change this, nor does changing the size of the hidden layer...

I had to put a l limit of frames without any goal being scored as P1 and P2 would regularly get stuck at the bottom justpining the ball back and forwards
at a shallow angel without having to move...

IDEAS:
------
Change the states from pixel distances x,y to simple binary values:
state variable #1:
-1 if ball y is less than agent (P1) paddle y upper end
0 if ball is same y as paddle (i.e. whole length of paddle)
+1 if ball y is greater than paddle y lower end

state variable #2:
direction of ball
0 away from agent(P1)
1 toward agent (P1)

so example states
[0,0]   ball on same y as paddle, but mving away
[-1,1]  ball below paddle and moving towards paddle
